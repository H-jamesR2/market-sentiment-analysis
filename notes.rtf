First [Architecture]:

market-sentiment-analysis/
â”‚â”€â”€ backend/                     # Spring Boot Backend (Java)
â”‚   â”œâ”€â”€ src/main/java/com/example/marketanalysis/
â”‚   â”‚   â”œâ”€â”€ config/              # Configurations (Kafka, Cassandra, etc.)
â”‚   â”‚   â”œâ”€â”€ controller/          # REST API Controllers
â”‚   â”‚   â”œâ”€â”€ model/               # Data Models (Cassandra Entities)
â”‚   â”‚   â”œâ”€â”€ repository/          # DAO Layer (Cassandra Repositories)
â”‚   â”‚   â”œâ”€â”€ service/             # Business Logic (Kafka Producers, Spark Job Triggers)
â”‚   â”‚   â”œâ”€â”€ util/                # Utility Functions (Data Parsing, Common Helpers)
â”‚   â”‚   â”œâ”€â”€ MarketAnalysisApplication.java  # Main Entry Point
â”‚   â”œâ”€â”€ pom.xml                   # Maven Dependencies
â”‚   â”œâ”€â”€ Dockerfile                 # Containerization
â”‚
â”‚â”€â”€ ingestion/                     # Kafka Producers (Java)
â”‚   â”œâ”€â”€ src/main/java/com/example/ingestion/
â”‚   â”‚   â”œâ”€â”€ TwitterProducer.java  # Twitter API to Kafka
â”‚   â”‚   â”œâ”€â”€ RedditProducer.java   # Reddit API to Kafka
â”‚   â”‚   â”œâ”€â”€ NewsProducer.java     # Yahoo Finance Scraper
â”‚   â”‚   â”œâ”€â”€ SECProducer.java      # SEC Filings Ingestion
â”‚   â”œâ”€â”€ pom.xml                    # Maven Dependencies
â”‚
â”‚â”€â”€ processing/                     # Spark Streaming & Sentiment Analysis
â”‚   â”œâ”€â”€ src/main/java/com/example/processing/
â”‚   â”‚   â”œâ”€â”€ SparkSentimentProcessor.java  # Spark Kafka Consumer
â”‚   â”‚   â”œâ”€â”€ SentimentAnalyzer.java       # NLP Model (Minimal ML Dependency)
â”‚   â”œâ”€â”€ pom.xml                          # Dependencies (Spark, NLP)
â”‚
â”‚â”€â”€ cassandra/                     # NoSQL Database Setup
â”‚   â”œâ”€â”€ schema.cql                  # Cassandra Table Definitions
â”‚
â”‚â”€â”€ dashboard/                      # Optional (React Frontend for Visualization)
â”‚   â”œâ”€â”€ src/                         # React Code
â”‚   â”œâ”€â”€ package.json                 # Frontend Dependencies
â”‚
â”‚â”€â”€ docker-compose.yml               # Infrastructure Setup (Kafka, Cassandra, Spark)
â”‚â”€â”€ README.md                         # Documentation

Next, [Ingestion Services]
    ingestion/
    â”‚â”€â”€ src/main/java/com/example/ingestion/
    â”‚   â”œâ”€â”€ reddit/
    â”‚   â”‚   â”œâ”€â”€ RedditProducer.java          # Main Kafka producer
    â”‚   â”‚   â”œâ”€â”€ RedditClient.java            # Handles API requests
    â”‚   â”‚   â”œâ”€â”€ RedditParser.java            # Extracts relevant fields
    â”‚   â”‚   â”œâ”€â”€ RedditConfig.java            # Config loader
    â”‚   â”œâ”€â”€ kafka/
    â”‚   â”‚   â”œâ”€â”€ KafkaProducerService.java    # Generic Kafka producer logic
    â”‚   â”œâ”€â”€ util/
    â”‚   â”‚   â”œâ”€â”€ JsonUtil.java                # Lightweight JSON helper
    â”‚   â”œâ”€â”€ Main.java                         # Entry point
    â”‚â”€â”€ resources/
    â”‚   â”œâ”€â”€ application.properties           # Config (subreddits, Kafka settings)
    â”‚â”€â”€ pom.xml                               # Dependencies (minimal)

KAFKA Topics:
    reddit_posts


==================
[Terminal instance 1]
Setup to run kafka before running ingestion service [@ root directory]:
    docker system prune -a --volumes -f
    docker system df

    docker-compose up -d
    docker ps | grep kafka #check if running
    docker exec -it market-sentiment-analysis-kafka-1 kafka-topics.sh --bootstrap-server localhost:9092 --list
    docker exec -it market-sentiment-analysis-kafka-1 kafka-console-consumer.sh --bootstrap-server \
    localhost:9092 --topic reddit_posts --from-beginning

    docker exec -it market-sentiment-analysis-kafka-1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic reddit_posts --from-beginning
    docker exec -it market-sentiment-analysis-kafka-1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic reddit_comments --from-beginning

    docker exec -it market-sentiment-analysis-kafka-1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic __consumer_offsets


==================
[Terminal instance 2]
When testing code [running the following commands]:
    mvn clean install
    mvn spring-boot:run -X
    -> need to change to directory first [ingestion, backend, etc..]

Cleaning up maven_dependencies:
    mvn clean
    mvn dependency:purge-local-repository
    mvn dependency:tree


=========
For KAFKA:
Since Kafka setup is running inside Docker,
    shouldnâ€™t need to install Kafka manually.
The issue is that `kafka-topics.sh` and `kafka-console-consumer.sh`
    arenâ€™t available on your **host machine**â€”they exist inside the Kafka container.

### **1. List Running Kafka Containers**
Run this to check if Kafka is running in Docker:
```bash
docker ps --filter "name=kafka"
```
If nothing shows up, your Kafka container isnâ€™t running.

---

### **2. Get Kafka Container Name**
If Kafka **is** running, find its container name:
```bash
docker ps --format "{{.Names}}"
```
Look for something like `kafka_broker_1` or whatever name you gave it.

---

### **3. Run Kafka CLI Inside the Container**
Once you have the container name (letâ€™s say itâ€™s `kafka_broker_1`),
[
market-sentiment-analysis-kafka-1
market-sentiment-analysis-zookeeper-1
]
exec into it:
```bash
docker exec -it kafka_broker_1 bash
docker exec -it market-sentiment-analysis-kafka-1 bash
```
Then, inside the container, run:
```bash
kafka-topics.sh --bootstrap-server localhost:9092 --list
```
or
```bash
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <your_topic_name> --from-beginning
```

---

### **4. Alternative: Run Commands Without Entering the Container**
Instead of entering the container, you can run commands like this:
```bash
docker exec -it kafka_broker_1 kafka-topics.sh --bootstrap-server localhost:9092 --list
```
or
```bash
docker exec -it kafka_broker_1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <your_topic_name> --from-beginning
```

---

### **Next Steps**
- If no topics show up, your producer might not be sending data correctly.
- If the container isnâ€™t running, we might need to restart Kafka using `docker-compose`.

=========================

For handling duplicate messages,
need to define a strategy based on how we treat updates.

Some possible approaches:
    1. **Deduplication at the Producer Level**
       - Store the latest known state of posts (e.g., in an in-memory cache or external store like Redis).
       - Before sending a new message, check if it differs significantly from the last stored version.
       - If only upvotes change, decide whether to send an update or ignore it.

    2. **Deduplication at the Consumer Level**
       - Let Kafka consume all messages but introduce logic in the consumer to discard updates with minimal changes.
       - Store seen messages in a cache or database and compare before processing.

    3. **Kafka Compaction**
       - Kafka has a log compaction feature where only the most recent message per key (e.g., `post_id`) is retained.
       - Useful if you want to keep only the latest version of a post in Kafka.

For `RedditComment`, itâ€™ll have similar challenges
    (edited comments, changing upvotes, deleted comments).
    Likely need logic to determine whether a comment update is worth processing.

Before we move to `RedditComment`,
    let's clean up `DataFetchScheduler` and `KafkaProducerService`.


=====================
If we factor in market sentiment, we might need to:
- Assign **different weights** to **posts vs. comments**
    (e.g., high-upvote comments might be more valuable than low-engagement posts).
- Decide **how often to track upvotes** (do we care about every change or just major shifts?).
- Consider **sentiment analysis on edits** (does the sentiment of a post/comment change over time?).

For now,
- refactor `DataFetchScheduler` and `KafkaProducerService`.

---

### **Refactoring Goals**
1. **Make `DataFetchScheduler` more extensible**
   - Ensure it can fetch from multiple data sources (not just Reddit).
   - Allow scheduling logic to be configurable.

2. **Improve `KafkaProducerService`**
   - Decouple it from being Reddit-specific.
   - Improve reusability and logging.

---

### **Plan for `DataFetchScheduler`**
- Introduce a **`DataSourceFetcher` interface** that different sources (Reddit, Twitter, etc.) can implement.
- Modify `DataFetchScheduler` to **loop through multiple fetchers** instead of hardcoding Reddit logic.
- Move scheduling configuration (e.g., fetch interval) to `application.properties`.

---

### **Plan for `KafkaProducerService`**
- Introduce a **generic `produceMessage(topic, key, value)` method** instead of tying it to Reddit data.
- Improve logging to track when messages are dropped (e.g., due to deduplication logic).
- Make it easier to plug in **message transformation or enrichment** if needed later.

---

======================
### **ğŸ“Œ Where Should Redis Deduplication Go? (Ingestion vs. Backend)**

Since **Redis Deduplication** is primarily used for **filtering redundant data before pushing to Kafka**,
    it makes more sense to **keep it inside `ingestion/`** rather than `backend/`.

### **ğŸ” Considerations**
| **Factor** | **Ingestion** âœ… | **Backend** âŒ |
|------------|----------------|---------------|
| **Purpose** | Ingestion filters and pre-processes data before Kafka. | Backend handles analytics, queries, and market insights. |
| **Data Flow** | Deduplication happens **before Kafka** to avoid sending redundant messages. | Backend consumes from Kafka and does not control ingestion. |
| **Redis Use Case** | Used for short-term caching of processed posts/comments to prevent duplicate ingestion. | Typically used for analytics caching, user sessions, or recommendation storage. |
| **Scalability** | Keeping deduplication in ingestion **reduces load on Kafka & backend**. | If done in backend, duplicate messages would already be processed and stored, leading to inefficiencies. |

---

### **âœ… Recommendation: Keep Deduplication in `ingestion/`**
- **Move `DeduplicationService` to `ingestion/services/`** (already structured for this).
- **Future-proofing**: If backend needs deduplication for **query caching**
    (e.g., preventing duplicate analysis runs), it can have **a separate Redis instance or namespace**.
- **Backend remains focused on analytics and querying**,
    while ingestion handles **data quality before sending to Kafka**.

---

### **ğŸ›  Updated Repository Structure**
```
ingestion [my-project]/
â”‚â”€â”€ src/main/java/com.example.ingestion/
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ fetcher/
â”‚   â”œâ”€â”€ messaging/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ parsers/
â”‚   â”œâ”€â”€ producers/
â”‚   â”œâ”€â”€ scheduler/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ DeduplicationService.java  <-- (ğŸ”¹ Add here)
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ Main.java
â”‚â”€â”€ resources/
â”‚     â”œâ”€â”€ application.properties
â”‚â”€â”€ pom.xml

backend/
â”‚â”€â”€ src/main/java/com.example.marketanalysis/
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ controller/
â”‚   â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ repository/
â”‚   â”œâ”€â”€ service/
â”‚   â”œâ”€â”€ util/
â”‚   â”œâ”€â”€ MarketAnalysisApplication.java
â”‚â”€â”€ Dockerfile
â”‚â”€â”€ pom.xml
```

---
### **â“ Do We Even Need the `backend/` Folder?**
We might not need `backend/` at all **unless**:
- We plan to **build an API** for querying the processed market sentiment data.
- We need **data visualization, user interaction, or REST endpoints**.
- We want to **store aggregated insights in a database** for external access.

If our goal is **just Kafka â†’ Spark â†’ Market Sentiment Analysis**,
then the ingestion pipeline **alone may be sufficient** for now.

