First [Architecture]:

market-sentiment-analysis/
‚îÇ‚îÄ‚îÄ backend/                     # Spring Boot Backend (Java)
‚îÇ   ‚îú‚îÄ‚îÄ src/main/java/com/example/marketanalysis/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config/              # Configurations (Kafka, Cassandra, etc.)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ controller/          # REST API Controllers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model/               # Data Models (Cassandra Entities)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ repository/          # DAO Layer (Cassandra Repositories)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service/             # Business Logic (Kafka Producers, Spark Job Triggers)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ util/                # Utility Functions (Data Parsing, Common Helpers)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MarketAnalysisApplication.java  # Main Entry Point
‚îÇ   ‚îú‚îÄ‚îÄ pom.xml                   # Maven Dependencies
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                 # Containerization
‚îÇ
‚îÇ‚îÄ‚îÄ ingestion/                     # Kafka Producers (Java)
‚îÇ   ‚îú‚îÄ‚îÄ src/main/java/com/example/ingestion/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TwitterProducer.java  # Twitter API to Kafka
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RedditProducer.java   # Reddit API to Kafka
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ NewsProducer.java     # Yahoo Finance Scraper
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SECProducer.java      # SEC Filings Ingestion
‚îÇ   ‚îú‚îÄ‚îÄ pom.xml                    # Maven Dependencies
‚îÇ
‚îÇ‚îÄ‚îÄ processing/                     # Spark Streaming & Sentiment Analysis
‚îÇ   ‚îú‚îÄ‚îÄ src/main/java/com/example/processing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SparkSentimentProcessor.java  # Spark Kafka Consumer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SentimentAnalyzer.java       # NLP Model (Minimal ML Dependency)
‚îÇ   ‚îú‚îÄ‚îÄ pom.xml                          # Dependencies (Spark, NLP)
‚îÇ
‚îÇ‚îÄ‚îÄ cassandra/                     # NoSQL Database Setup
‚îÇ   ‚îú‚îÄ‚îÄ schema.cql                  # Cassandra Table Definitions
‚îÇ
‚îÇ‚îÄ‚îÄ dashboard/                      # Optional (React Frontend for Visualization)
‚îÇ   ‚îú‚îÄ‚îÄ src/                         # React Code
‚îÇ   ‚îú‚îÄ‚îÄ package.json                 # Frontend Dependencies
‚îÇ
‚îÇ‚îÄ‚îÄ docker-compose.yml               # Infrastructure Setup (Kafka, Cassandra, Spark)
‚îÇ‚îÄ‚îÄ README.md                         # Documentation

Next, [Ingestion Services]
    ingestion/
    ‚îÇ‚îÄ‚îÄ src/main/java/com/example/ingestion/
    ‚îÇ   ‚îú‚îÄ‚îÄ reddit/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RedditProducer.java          # Main Kafka producer
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RedditClient.java            # Handles API requests
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RedditParser.java            # Extracts relevant fields
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RedditConfig.java            # Config loader
    ‚îÇ   ‚îú‚îÄ‚îÄ kafka/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ KafkaProducerService.java    # Generic Kafka producer logic
    ‚îÇ   ‚îú‚îÄ‚îÄ util/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ JsonUtil.java                # Lightweight JSON helper
    ‚îÇ   ‚îú‚îÄ‚îÄ Main.java                         # Entry point
    ‚îÇ‚îÄ‚îÄ resources/
    ‚îÇ   ‚îú‚îÄ‚îÄ application.properties           # Config (subreddits, Kafka settings)
    ‚îÇ‚îÄ‚îÄ pom.xml                               # Dependencies (minimal)

KAFKA Topics:
    reddit_posts


==================
[Terminal instance 1]
Setup to run kafka before running ingestion service [@ root directory]:
    docker system prune -a --volumes -f
    docker system df

    docker-compose up -d
    docker ps | grep kafka #check if running
    docker exec -it market-sentiment-analysis-kafka-1 kafka-topics.sh --bootstrap-server localhost:9092 --list
    docker exec -it market-sentiment-analysis-kafka-1 kafka-console-consumer.sh --bootstrap-server \
    localhost:9092 --topic reddit_posts --from-beginning

    docker exec -it market-sentiment-analysis-kafka-1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic reddit_posts --from-beginning
    docker exec -it market-sentiment-analysis-kafka-1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic reddit_comments --from-beginning

    docker exec -it market-sentiment-analysis-kafka-1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic __consumer_offsets


==================
[Terminal instance 2]
When testing code [running the following commands]:
    mvn clean install
    mvn spring-boot:run -X
    -> need to change to directory first [ingestion, backend, etc..]

Cleaning up maven_dependencies:
    mvn clean
    mvn dependency:purge-local-repository
    mvn dependency:tree


=========
For KAFKA:
Since Kafka setup is running inside Docker,
    shouldn‚Äôt need to install Kafka manually.
The issue is that `kafka-topics.sh` and `kafka-console-consumer.sh`
    aren‚Äôt available on your **host machine**‚Äîthey exist inside the Kafka container.

### **1. List Running Kafka Containers**
Run this to check if Kafka is running in Docker:
```bash
docker ps --filter "name=kafka"
```
If nothing shows up, your Kafka container isn‚Äôt running.

---

### **2. Get Kafka Container Name**
If Kafka **is** running, find its container name:
```bash
docker ps --format "{{.Names}}"
```
Look for something like `kafka_broker_1` or whatever name you gave it.

---

### **3. Run Kafka CLI Inside the Container**
Once you have the container name (let‚Äôs say it‚Äôs `kafka_broker_1`),
[
market-sentiment-analysis-kafka-1
market-sentiment-analysis-zookeeper-1
]
exec into it:
```bash
docker exec -it kafka_broker_1 bash
docker exec -it market-sentiment-analysis-kafka-1 bash
```
Then, inside the container, run:
```bash
kafka-topics.sh --bootstrap-server localhost:9092 --list
```
or
```bash
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <your_topic_name> --from-beginning
```

---

### **4. Alternative: Run Commands Without Entering the Container**
Instead of entering the container, you can run commands like this:
```bash
docker exec -it kafka_broker_1 kafka-topics.sh --bootstrap-server localhost:9092 --list
```
or
```bash
docker exec -it kafka_broker_1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <your_topic_name> --from-beginning
```

---

### **Next Steps**
- If no topics show up, your producer might not be sending data correctly.
- If the container isn‚Äôt running, we might need to restart Kafka using `docker-compose`.

=========================

For handling duplicate messages,
need to define a strategy based on how we treat updates.

Some possible approaches:
    1. **Deduplication at the Producer Level**
       - Store the latest known state of posts (e.g., in an in-memory cache or external store like Redis).
       - Before sending a new message, check if it differs significantly from the last stored version.
       - If only upvotes change, decide whether to send an update or ignore it.

    2. **Deduplication at the Consumer Level**
       - Let Kafka consume all messages but introduce logic in the consumer to discard updates with minimal changes.
       - Store seen messages in a cache or database and compare before processing.

    3. **Kafka Compaction**
       - Kafka has a log compaction feature where only the most recent message per key (e.g., `post_id`) is retained.
       - Useful if you want to keep only the latest version of a post in Kafka.

For `RedditComment`, it‚Äôll have similar challenges
    (edited comments, changing upvotes, deleted comments).
    Likely need logic to determine whether a comment update is worth processing.

Before we move to `RedditComment`,
    let's clean up `DataFetchScheduler` and `KafkaProducerService`.


=====================
If we factor in market sentiment, we might need to:
- Assign **different weights** to **posts vs. comments**
    (e.g., high-upvote comments might be more valuable than low-engagement posts).
- Decide **how often to track upvotes** (do we care about every change or just major shifts?).
- Consider **sentiment analysis on edits** (does the sentiment of a post/comment change over time?).

For now,
- refactor `DataFetchScheduler` and `KafkaProducerService`.

---

### **Refactoring Goals**
1. **Make `DataFetchScheduler` more extensible**
   - Ensure it can fetch from multiple data sources (not just Reddit).
   - Allow scheduling logic to be configurable.

2. **Improve `KafkaProducerService`**
   - Decouple it from being Reddit-specific.
   - Improve reusability and logging.

---

### **Plan for `DataFetchScheduler`**
- Introduce a **`DataSourceFetcher` interface** that different sources (Reddit, Twitter, etc.) can implement.
- Modify `DataFetchScheduler` to **loop through multiple fetchers** instead of hardcoding Reddit logic.
- Move scheduling configuration (e.g., fetch interval) to `application.properties`.

---

### **Plan for `KafkaProducerService`**
- Introduce a **generic `produceMessage(topic, key, value)` method** instead of tying it to Reddit data.
- Improve logging to track when messages are dropped (e.g., due to deduplication logic).
- Make it easier to plug in **message transformation or enrichment** if needed later.

---

======================
### **üìå Where Should Redis Deduplication Go? (Ingestion vs. Backend)**

Since **Redis Deduplication** is primarily used for **filtering redundant data before pushing to Kafka**,
    it makes more sense to **keep it inside `ingestion/`** rather than `backend/`.

### **üîç Considerations**
| **Factor** | **Ingestion** ‚úÖ | **Backend** ‚ùå |
|------------|----------------|---------------|
| **Purpose** | Ingestion filters and pre-processes data before Kafka. | Backend handles analytics, queries, and market insights. |
| **Data Flow** | Deduplication happens **before Kafka** to avoid sending redundant messages. | Backend consumes from Kafka and does not control ingestion. |
| **Redis Use Case** | Used for short-term caching of processed posts/comments to prevent duplicate ingestion. | Typically used for analytics caching, user sessions, or recommendation storage. |
| **Scalability** | Keeping deduplication in ingestion **reduces load on Kafka & backend**. | If done in backend, duplicate messages would already be processed and stored, leading to inefficiencies. |

---

### **‚úÖ Recommendation: Keep Deduplication in `ingestion/`**
- **Move `DeduplicationService` to `ingestion/services/`** (already structured for this).
- **Future-proofing**: If backend needs deduplication for **query caching**
    (e.g., preventing duplicate analysis runs), it can have **a separate Redis instance or namespace**.
- **Backend remains focused on analytics and querying**,
    while ingestion handles **data quality before sending to Kafka**.

---

### **üõ† Updated Repository Structure**
```
ingestion [my-project]/
‚îÇ‚îÄ‚îÄ src/main/java/com.example.ingestion/
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ fetcher/
‚îÇ   ‚îú‚îÄ‚îÄ messaging/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ parsers/
‚îÇ   ‚îú‚îÄ‚îÄ producers/
‚îÇ   ‚îú‚îÄ‚îÄ scheduler/
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DeduplicationService.java  <-- (üîπ Add here)
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ Main.java
‚îÇ‚îÄ‚îÄ resources/
‚îÇ     ‚îú‚îÄ‚îÄ application.properties
‚îÇ‚îÄ‚îÄ pom.xml

backend/
‚îÇ‚îÄ‚îÄ src/main/java/com.example.marketanalysis/
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ controller/
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ repository/
‚îÇ   ‚îú‚îÄ‚îÄ service/
‚îÇ   ‚îú‚îÄ‚îÄ util/
‚îÇ   ‚îú‚îÄ‚îÄ MarketAnalysisApplication.java
‚îÇ‚îÄ‚îÄ Dockerfile
‚îÇ‚îÄ‚îÄ pom.xml
```

---
### **‚ùì Do We Even Need the `backend/` Folder?**
We might not need `backend/` at all **unless**:
- We plan to **build an API** for querying the processed market sentiment data.
- We need **data visualization, user interaction, or REST endpoints**.
- We want to **store aggregated insights in a database** for external access.

If our goal is **just Kafka ‚Üí Spark ‚Üí Market Sentiment Analysis**,
then the ingestion pipeline **alone may be sufficient** for now.

