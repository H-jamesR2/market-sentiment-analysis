First [Architecture]:

market-sentiment-analysis/
│── backend/                     # Spring Boot Backend (Java)
│   ├── src/main/java/com/example/marketanalysis/
│   │   ├── config/              # Configurations (Kafka, Cassandra, etc.)
│   │   ├── controller/          # REST API Controllers
│   │   ├── model/               # Data Models (Cassandra Entities)
│   │   ├── repository/          # DAO Layer (Cassandra Repositories)
│   │   ├── service/             # Business Logic (Kafka Producers, Spark Job Triggers)
│   │   ├── util/                # Utility Functions (Data Parsing, Common Helpers)
│   │   ├── MarketAnalysisApplication.java  # Main Entry Point
│   ├── pom.xml                   # Maven Dependencies
│   ├── Dockerfile                 # Containerization
│
│── ingestion/                     # Kafka Producers (Java)
│   ├── src/main/java/com/example/ingestion/
│   │   ├── TwitterProducer.java  # Twitter API to Kafka
│   │   ├── RedditProducer.java   # Reddit API to Kafka
│   │   ├── NewsProducer.java     # Yahoo Finance Scraper
│   │   ├── SECProducer.java      # SEC Filings Ingestion
│   ├── pom.xml                    # Maven Dependencies
│
│── processing/                     # Spark Streaming & Sentiment Analysis
│   ├── src/main/java/com/example/processing/
│   │   ├── SparkSentimentProcessor.java  # Spark Kafka Consumer
│   │   ├── SentimentAnalyzer.java       # NLP Model (Minimal ML Dependency)
│   ├── pom.xml                          # Dependencies (Spark, NLP)
│
│── cassandra/                     # NoSQL Database Setup
│   ├── schema.cql                  # Cassandra Table Definitions
│
│── dashboard/                      # Optional (React Frontend for Visualization)
│   ├── src/                         # React Code
│   ├── package.json                 # Frontend Dependencies
│
│── docker-compose.yml               # Infrastructure Setup (Kafka, Cassandra, Spark)
│── README.md                         # Documentation

Next, [Ingestion Services]
    ingestion/
    │── src/main/java/com/example/ingestion/
    │   ├── reddit/
    │   │   ├── RedditProducer.java          # Main Kafka producer
    │   │   ├── RedditClient.java            # Handles API requests
    │   │   ├── RedditParser.java            # Extracts relevant fields
    │   │   ├── RedditConfig.java            # Config loader
    │   ├── kafka/
    │   │   ├── KafkaProducerService.java    # Generic Kafka producer logic
    │   ├── util/
    │   │   ├── JsonUtil.java                # Lightweight JSON helper
    │   ├── Main.java                         # Entry point
    │── resources/
    │   ├── application.properties           # Config (subreddits, Kafka settings)
    │── pom.xml                               # Dependencies (minimal)

KAFKA Topics:
    reddit_posts


==================
[Terminal instance 1]
Setup to run kafka before running ingestion service [@ root directory]:
    docker system prune -a --volumes -f
    docker system df

    docker-compose up -d
    docker ps | grep kafka #check if running
    docker exec -it market-sentiment-analysis-kafka-1 kafka-topics.sh --bootstrap-server localhost:9092 --list
    docker exec -it market-sentiment-analysis-kafka-1 kafka-console-consumer.sh --bootstrap-server \
    localhost:9092 --topic reddit_posts --from-beginning

    docker exec -it market-sentiment-analysis-kafka-1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic reddit_posts --from-beginning

    docker exec -it market-sentiment-analysis-kafka-1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic __consumer_offsets


==================
[Terminal instance 2]
When testing code [running the following commands]:
    mvn clean install
    mvn spring-boot:run -X
    -> need to change to directory first [ingestion, backend, etc..]

Cleaning up maven_dependencies:
    mvn clean
    mvn dependency:purge-local-repository
    mvn dependency:tree


=========
For KAFKA:
Since Kafka setup is running inside Docker,
    shouldn’t need to install Kafka manually.
The issue is that `kafka-topics.sh` and `kafka-console-consumer.sh`
    aren’t available on your **host machine**—they exist inside the Kafka container.

### **1. List Running Kafka Containers**
Run this to check if Kafka is running in Docker:
```bash
docker ps --filter "name=kafka"
```
If nothing shows up, your Kafka container isn’t running.

---

### **2. Get Kafka Container Name**
If Kafka **is** running, find its container name:
```bash
docker ps --format "{{.Names}}"
```
Look for something like `kafka_broker_1` or whatever name you gave it.

---

### **3. Run Kafka CLI Inside the Container**
Once you have the container name (let’s say it’s `kafka_broker_1`),
[
market-sentiment-analysis-kafka-1
market-sentiment-analysis-zookeeper-1
]
exec into it:
```bash
docker exec -it kafka_broker_1 bash
docker exec -it market-sentiment-analysis-kafka-1 bash
```
Then, inside the container, run:
```bash
kafka-topics.sh --bootstrap-server localhost:9092 --list
```
or
```bash
kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <your_topic_name> --from-beginning
```

---

### **4. Alternative: Run Commands Without Entering the Container**
Instead of entering the container, you can run commands like this:
```bash
docker exec -it kafka_broker_1 kafka-topics.sh --bootstrap-server localhost:9092 --list
```
or
```bash
docker exec -it kafka_broker_1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <your_topic_name> --from-beginning
```

---

### **Next Steps**
- If no topics show up, your producer might not be sending data correctly.
- If the container isn’t running, we might need to restart Kafka using `docker-compose`.

=========================

For handling duplicate messages,
need to define a strategy based on how we treat updates.

Some possible approaches:
    1. **Deduplication at the Producer Level**
       - Store the latest known state of posts (e.g., in an in-memory cache or external store like Redis).
       - Before sending a new message, check if it differs significantly from the last stored version.
       - If only upvotes change, decide whether to send an update or ignore it.

    2. **Deduplication at the Consumer Level**
       - Let Kafka consume all messages but introduce logic in the consumer to discard updates with minimal changes.
       - Store seen messages in a cache or database and compare before processing.

    3. **Kafka Compaction**
       - Kafka has a log compaction feature where only the most recent message per key (e.g., `post_id`) is retained.
       - Useful if you want to keep only the latest version of a post in Kafka.

For `RedditComment`, it’ll have similar challenges
    (edited comments, changing upvotes, deleted comments).
    Likely need logic to determine whether a comment update is worth processing.

Before we move to `RedditComment`,
    let's clean up `DataFetchScheduler` and `KafkaProducerService`.


=====================
If we factor in market sentiment, we might need to:
- Assign **different weights** to **posts vs. comments**
    (e.g., high-upvote comments might be more valuable than low-engagement posts).
- Decide **how often to track upvotes** (do we care about every change or just major shifts?).
- Consider **sentiment analysis on edits** (does the sentiment of a post/comment change over time?).

For now,
- refactor `DataFetchScheduler` and `KafkaProducerService`.

---

### **Refactoring Goals**
1. **Make `DataFetchScheduler` more extensible**
   - Ensure it can fetch from multiple data sources (not just Reddit).
   - Allow scheduling logic to be configurable.

2. **Improve `KafkaProducerService`**
   - Decouple it from being Reddit-specific.
   - Improve reusability and logging.

---

### **Plan for `DataFetchScheduler`**
- Introduce a **`DataSourceFetcher` interface** that different sources (Reddit, Twitter, etc.) can implement.
- Modify `DataFetchScheduler` to **loop through multiple fetchers** instead of hardcoding Reddit logic.
- Move scheduling configuration (e.g., fetch interval) to `application.properties`.

---

### **Plan for `KafkaProducerService`**
- Introduce a **generic `produceMessage(topic, key, value)` method** instead of tying it to Reddit data.
- Improve logging to track when messages are dropped (e.g., due to deduplication logic).
- Make it easier to plug in **message transformation or enrichment** if needed later.

---

